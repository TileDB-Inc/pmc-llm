{"metadata": {"language_info": {"name": "python", "version": "3.9.19", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3 (ipykernel)", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# PMC RAG\n## Why RAG\nBy embracing RAG, you can unlock a range of benefits for your organization:\n\n * Improved decision-making: Accessing rights and trustworthy information empowers better choices and strategies.\n * Enhanced customer experience: Delivering reliable answers and insights builds trust and satisfaction.\n * Reduced risk and compliance: Curated data sources minimize the risk of misinformation and ensure compliance with regulations.\n * Increased efficiency: Streamlining access to information saves time and resources.\n \nThe beauty of RAG lies in its focus on data quality, not just data quantity. We're moving beyond the \u201cbigger is better\u201d mentality of massive models trained on internet data that often include misinformation and biases. RAG puts the emphasis on smaller, more valuable models that use curated, trustworthy data sources.\n", "metadata": {}, "id": "864ffbd4-ebb2-42c9-997f-0ef68f4d0fb8"}, {"cell_type": "markdown", "source": "## The Standard Rag Workflow Empowerd by TileDB\n\n1. **User:** Uploads documents, and the system converts them into vectors (numeric representations) using sentence embeddings.\n2. **User:** Stores these document vectors, along with the documents and metadata, into TileDB (a smart database for storing vectors).\n3. **User:** Asks a question.\n4. **Embedding Model:** Processes the user's question by embedding it into a vector and sends this vector to TileDB.\n5. **TileDB:** Searches through the stored document vectors and retrieves the most relevant documents.\n6. **System:** Takes these relevant documents and constructs a new query for the LLM, instructing it to use these documents as context.\n7. **LLM:** Uses the relevant documents as context to generate and deliver the final answer back to the user.", "metadata": {}, "id": "f983c8f8-d56d-4f5b-a51e-e5df2bb33982"}, {"cell_type": "markdown", "source": "## The Notebook and Dag Pipelines", "metadata": {}, "id": "00897ed6-52ec-4d4a-b116-2a286b990f68"}, {"cell_type": "markdown", "source": "### Imports\nThe below imports are for our local file push to our remote repo as well as the `initialize_step()` that will build our end to end pipeline for us and submit a dag directly. ", "metadata": {}, "id": "f4899111-8920-4850-99f8-666d406e7154"}, {"cell_type": "code", "source": "import requests\nimport os\nimport pandas as pd\nimport tarfile\nimport urllib.request\nimport xml.etree.ElementTree as ET\nimport tiledb\nfrom tiledb.cloud.dag import dag\nimport hashlib\nimport tiledb\nimport subprocess\nimport shutil\nimport math", "metadata": {"trusted": true}, "execution_count": 10, "outputs": [], "id": "c67374d0-369e-44e1-8a61-036b9a20f7b3"}, {"cell_type": "markdown", "source": "### Credentials\nThe below cell is so we can cache our credentials during an initial push for our steps. After you push the local file, you may need to manually enter creds and push the file from the terminal. Afterward, the credentials should be cached and you can run without issues. ", "metadata": {}, "id": "a39d443d-6d54-4d4a-9107-b5a8ce69ad81"}, {"cell_type": "code", "source": "#before running below please run this. The first push to the repo (if necessary) may fail and you will need to manually push thie file.\n#after, the credentials should be temporarily cached. \n#!git config --global user.email \"you@example.com\"\n#!git config --global user.name \"Your Name\"\n!git config --global credential.helper cache\n!git config --global credential.helper 'cache --timeout=3600'\n!git config --global credential.helper store", "metadata": {"trusted": true}, "execution_count": 11, "outputs": [], "id": "ab801d52-fc16-4a07-ad02-4c75b8e301dd"}, {"cell_type": "markdown", "source": "### Upload \"pipeline\" Helpers\nFor now, this is a quick simulation of a pipeline for updating the local file. The first cell is full ofhelper functions for our notebook pipeline (ran in our notebook). ", "metadata": {}, "id": "558a5935-9d69-44f8-96cb-a9ad27963199"}, {"cell_type": "code", "source": "import os\nimport subprocess\nimport hashlib\nimport pandas as pd\nimport math\n\ndef add_and_commit_files(message: str):\n    \"\"\"\n    Adds all files to the Git staging area, commits them with a provided message, and pushes the changes to the remote repository.\n    \n    :param message: Commit message to be used in the Git commit.\n    \"\"\"\n    \n    # Print the current working directory\n    print(f\"Working directory is {os.getcwd()}\")\n\n    # Stage all changes (new, modified, deleted) in the current Git repository\n    subprocess.run([\"git\", \"add\", \"-A\"])\n\n    # Commit the staged changes with the provided commit message\n    subprocess.run([\"git\", \"commit\", \"-m\", f\"{message}\"])\n\n    # Push the committed changes to the remote repository (origin/master by default)\n    subprocess.run([\"git\", \"push\"])\n\ndef hash_file(file_path: str) -> str:\n    \"\"\"\n    Computes the SHA256 hash of the contents of a file.\n\n    :param file_path: Path to the file to be hashed.\n    :return: The SHA256 hash of the file contents as a hex string.\n    \"\"\"\n    \n    # Create a new SHA256 hash object\n    hasher = hashlib.sha256()\n    \n    # Open the file in binary read mode\n    with open(file_path, 'rb') as f:\n        # Read the file contents and update the hash object\n        buffer = f.read()\n        hasher.update(buffer)\n    \n    # Return the hexadecimal digest of the hash\n    return hasher.hexdigest()\n\ndef estimate_resources(job_df: pd.DataFrame):\n    \"\"\"\n    Estimates the CPU and memory usage required for processing a given DataFrame, with an additional 2 GB overhead, \n    and rounds the total memory usage to the nearest GB.\n\n    :param job_df: A pandas DataFrame representing job data.\n    :return: A tuple containing the estimated number of CPUs per job and the total memory usage in GB (rounded).\n    \"\"\"\n    \n    # Print the data types of each column in the DataFrame for reference\n    print(\"Data types of the DataFrame:\")\n    print(job_df.dtypes)\n\n    # Calculate the memory usage of each column in the DataFrame in MB (deep=True considers the actual memory usage)\n    memory_usage_per_column = job_df.memory_usage(deep=True) / (1024 ** 2)  # Convert bytes to MB\n    total_memory_usage = memory_usage_per_column.sum()  # Sum of the memory usage of all columns\n    \n    # Convert total memory usage to GB and add an additional 2 GB overhead for processing\n    total_memory_usage_gb = total_memory_usage / 1024  # Convert MB to GB\n    total_memory_usage_gb += 2  # Add 2 GB overhead for processing\n    \n    # Round up the total memory usage to the nearest GB\n    rounded_memory_usage_gb = math.ceil(total_memory_usage_gb)\n    \n    # Print the memory usage for each column and the total estimated memory usage\n    print(\"Memory usage per column (MB):\")\n    print(memory_usage_per_column)\n    \n    print(f\"Total memory usage for job (GB, rounded to the nearest GB, with overhead): {rounded_memory_usage_gb}\")\n\n    # Estimate the CPU usage per job (adjustable based on job complexity)\n    cpu_per_job = 1  # Example: assuming 1 CPU per job\n    \n    # Return the estimated CPU count and the rounded memory usage in GB\n    return cpu_per_job, rounded_memory_usage_gb\n", "metadata": {"trusted": true}, "execution_count": 12, "outputs": [], "id": "12c33aa8-e715-46ba-9495-706a63e1194c"}, {"cell_type": "markdown", "source": "### The \"Upload Pipeline\" \nThis next cell is similar to a DevOps runner pipeline where a user would commit an updated file and the run would create a container with a run hash as the tag. The goal here is to determine if there is a change in the local file and update the hash. The next stage of the pipeline will/would use the hash to determine if a run is necessary. The frequency of checking a run really depends on the frequency of the file update and how we want to tune it to adjust the quality of our LLMs outputs. Fututre state this could be a webhook upon git update, or s3 storage. The benefit of git for this is the \"GitOps\" like workflow of tracking changes to our ingestion documents file for RAG and then using that knowledge to understand the impact on our model outputs. \n\n#### The Code Below in Plain English\n\n1. **Change to Home Directory:** It starts by navigating to the user's home directory.\n\n2. **Clone or Pull Repository:** It checks if a given repository (based on the URL) already exists locally. If it exists, it updates the repository by pulling the latest changes. If it doesn't exist, it clones the repository from the given URL.\n\n3. **Hash a Local File:** It calculates a hash (unique identifier) of a local file's contents to check if it has been modified.\n\n4. **Compare the Hash:** It checks whether a previously saved hash (in a hash.txt file) exists. If it does, the new hash is compared to the saved one to determine if the file has changed.\n\n5. **Skip or Proceed:** If the file hasn't changed (i.e., the current hash matches the previous one), it skips any further action. If the file has changed, it proceeds.\n\n6. **Update the Hash File:** It writes the new hash into a hash.txt file in the repository.\n\n7. **Commit Changes to Git:** Finally, it navigates to the repository directory, adds, commits, and pushes the new or modified file and updated hash to the Git repository", "metadata": {}, "id": "848ad50b-5fc7-4c57-9b42-038dc4f44807"}, {"cell_type": "code", "source": "import os\nimport subprocess\n\ndef handle_local_file(repo_url: str, local_file_name: str, hash_file_name: str):\n    \"\"\"\n    Clones or pulls a Git repository, checks if a local file has changed by comparing its hash with a stored hash,\n    and if the file has been modified, pushes the updated file and hash to the repository.\n    \n    :param repo_url: The URL of the Git repository.\n    :param local_file_name: The local file whose hash will be checked for modifications.\n    :param hash_file_name: The file in the repo that stores the previous hash of the local file.\n    \"\"\"\n    \n    # Step 1: Navigate to the home directory to clone or pull the repo\n    home_directory = os.path.expanduser(\"~\")  # Get the user's home directory\n    os.chdir(home_directory)  # Change the working directory to the home directory\n    \n    # Extract the repository name from the repo URL (assumes .git format at the end)\n    repo_name = repo_url.split('/')[-1].replace('.git', '')\n    \n    # Check if the repository already exists locally\n    if os.path.exists(repo_name): \n        # If the repo exists, navigate into it and pull the latest changes\n        os.chdir(repo_name)\n        subprocess.run([\"git\", \"pull\"])  # Pull latest changes from the remote repo\n        os.chdir(\"..\")  # Go back to the home directory after pulling changes\n    else:\n        # If the repo doesn't exist, clone it from the provided URL\n        subprocess.run([\"git\", \"clone\", repo_url])  # Clone the repository\n    \n    # Get the full path to the local repository\n    local_repo_path = os.path.join(os.getcwd(), repo_name)\n    print(f\"Created {local_repo_path}\")  # Print the path to the repo\n    \n    # Step 2: Hash the contents of the local file\n    current_hash = hash_file(local_file_name)  # Hash the local file\n    \n    # Step 3: Check if the hash file exists in the repository and compare hashes\n    hash_file_path = os.path.join(local_repo_path, hash_file_name)  # Path to the hash file in the repo\n    \n    if os.path.exists(hash_file_path):\n        # If the hash file exists, read the previous hash\n        with open(hash_file_path, 'r') as f:\n            previous_hash = f.read().strip()  # Strip any extra whitespace\n    else:\n        previous_hash = \"\"  # If the hash file doesn't exist, assume no previous hash\n    \n    # Step 4: Compare the current hash with the previous hash\n    if current_hash == previous_hash:\n        # If the hashes match, the file hasn't changed\n        print(\"File has not changed, skipping submission.\")\n        return\n    else:\n        # If the hashes differ, the file has changed, so proceed with updating the repo\n        print(\"File has changed, proceeding with submission.\")\n    \n    # Step 5: Update the hash file in the repository with the new hash\n    with open(hash_file_path, 'w') as f:\n        f.write(current_hash)  # Write the new hash to the file\n    \n    # Step 6: Commit the changes (the updated file and the new hash) to the Git repository\n    os.chdir(local_repo_path)  # Change directory to the repo\n    add_and_commit_files(\"added local articles file to the repository\")  # Stage, commit, and push the changes\n", "metadata": {"trusted": true}, "execution_count": 4, "outputs": [], "id": "4578b3e2-d8ff-4a0b-9e9a-7909dd0a0997"}, {"cell_type": "code", "source": "# Example usage\n#handle_local_file(\n#    repo_url=\"https://github.com/TileDB-Inc/pmc-llm.git\", \n#    local_file_name=\"rag-article-list.txt\", \n#    hash_file_name=\"hash.txt\"\n#)", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "46ece7fb-6197-40c6-be21-31cc2c30b5fd"}, {"cell_type": "markdown", "source": "## Ingestion Tasks for the TileDB DAG", "metadata": {}, "id": "aa8d563e-8148-4ac0-8ab7-d94ed0e713c9"}, {"cell_type": "code", "source": "def consolidate_chunks(total_jobs, bucket_region, object_path):\n    \"\"\"\n    Consolidates a list of chunked files into a single file in S3.\n\n    :param total_jobs: Number of chunked files to process\n    :param bucket_region: The S3 bucket region\n    :param object_path: The S3 path to the files\n    \"\"\"\n    import tiledb\n    \n    # Create a TileDB context with the specified S3 region\n    ctx = tiledb.Ctx({\"vfs.s3.region\": bucket_region})\n    # Initialize the TileDB VFS (Virtual File System) to interact with S3\n    vfs = tiledb.VFS(ctx=ctx)\n    \n    # Initialize tmp_data as an empty bytes object to store the consolidated data\n    tmp_data = b\"\"  # Use bytes since the files are being read in binary mode\n    \n    # Loop through all the chunked files and concatenate them\n    for i in range(total_jobs):\n        # Construct the file path for each chunk\n        file_path = f\"{object_path}/file_history_{i}\"\n        \n        # Check if the file exists in the S3 bucket\n        if vfs.is_file(file_path):\n            # If the file exists, open it in binary read mode and append its contents\n            with vfs.open(file_path, 'rb') as f:\n                tmp_data += f.read()  # Read and concatenate the binary content\n        else:\n            # If the file doesn't exist, print a message and continue to the next file\n            print(f\"No such file path {file_path}. Moving onto the next file.\")\n            continue\n    \n    # Now `tmp_data` contains the concatenated data from all chunked files\n    # Define the path for the consolidated file in the S3 bucket\n    consolidated_file_path = f\"{object_path}/consolidated_file_history\"\n    \n    # Open the consolidated file in binary write mode and write the consolidated data\n    with vfs.open(consolidated_file_path, 'wb') as f:\n        f.write(tmp_data)  # Write the concatenated binary data to the new file\n    \n    # Print a success message with the path of the consolidated file\n    print(f\"Consolidation complete. Consolidated file uploaded to: {consolidated_file_path}\")\n", "metadata": {"trusted": true}, "execution_count": 5, "outputs": [], "id": "2e31c91d-8683-4217-b65f-8e6b22eeff0f"}, {"cell_type": "code", "source": "def delete_unwanted_files(bucket_region, bucket_path):\n    \"\"\"\n    Deletes files in an S3 bucket that are not listed in the 'consolidated_file_history' file.\n    Also creates a few empty test files and skips deleting the 'consolidated_file_history' file.\n    \n    :param bucket_region: The S3 bucket region\n    :param bucket_path: The S3 path to the bucket\n    \"\"\"\n    import tiledb\n    import random\n    import string\n    import os\n    \n    # Initialize TileDB context and VFS with the specified S3 region\n    ctx = tiledb.Ctx({\"vfs.s3.region\": bucket_region})\n    vfs = tiledb.VFS(ctx=ctx)\n    \n    # Path to the 'consolidated_file_history' file in S3\n    consolidated_history_path = os.path.join(bucket_path, \"consolidated_file_history\")\n    \n    # Step 1: Load the list of valid files from 'consolidated_file_history'\n    valid_files = set()  # Initialize an empty set to store valid file names\n    if vfs.is_file(consolidated_history_path):\n        # If 'consolidated_file_history' exists, open and read it\n        with vfs.open(consolidated_history_path, 'rb') as f:\n            # Read each line, decode from bytes to string, and strip newline characters\n            valid_files = set(line.decode('utf-8').strip() for line in f.read().splitlines())\n    else:\n        # Raise an error if the 'consolidated_file_history' file is not found\n        raise FileNotFoundError(f\"{consolidated_history_path} not found.\")\n    \n    # Step 2: Create a few empty test files in the S3 bucket\n    for _ in range(5):\n        # Generate a random file name (e.g., 'Khaos_file_12345')\n        random_file_name = f\"Khaos_file_{''.join(random.choices(string.digits, k=5))}\"\n        random_file_path = os.path.join(bucket_path, random_file_name)\n        # Write the test file content in binary mode\n        with vfs.open(random_file_path, 'wb') as f:\n            test = \"I am an agent of Khaos here to ruin your RAG!!\"\n            f.write(test.encode('utf-8'))  # Encode the string before writing as bytes\n    \n    # Step 3: List all files in the S3 bucket\n    all_files = []  # Initialize a list to store all file paths\n    if vfs.is_dir(bucket_path):\n        # If the bucket is a directory, list all files\n        for file in vfs.ls(bucket_path):\n            if not file.endswith('/'):  # Skip directories\n                all_files.append(file)\n    \n    print(\"Begin the search!\")\n    \n    # Step 4: Delete unwanted files\n    for file_path in all_files:\n        # Extract the file name from the full path\n        file_name = os.path.basename(file_path)\n        \n        # Skip the 'consolidated_file_history' file to avoid deleting it\n        if file_name == \"consolidated_file_history\":\n            print(f\"Skipping deletion of {file_name}, as it is the consolidated file.\")\n            continue  # Skip further checks for this file\n        \n        # Check if the file name is NOT in the consolidated list, delete if not\n        if file_name not in valid_files:\n            print(f\"Deleting {file_path} because it's not in consolidated_file_history.\")\n            vfs.remove_file(file_path)  # Remove the file from S3\n        else:\n            print(f\"Keeping {file_path}, it's in consolidated_file_history.\")\n", "metadata": {"trusted": true}, "execution_count": 7, "outputs": [], "id": "19dbc31b-b3fa-44bd-a35f-666022bbe261"}, {"cell_type": "code", "source": "def pmid_ingestion(job_df, object_directory: str, bucket_region: str, job_id: int):\n    \"\"\"\n    Main function to handle the ingestion of articles using PubMed API and TileDB VFS.\n    \n    :param job_df: DataFrame containing PubMed PMIDs and gene-disease data.\n    :param object_directory: Path to the S3 bucket for file storage.\n    :param bucket_region: Region of the S3 bucket.\n    :param job_id: ID of the job to track file history.\n    \"\"\"\n    \n    import time\n    from datetime import datetime\n    import os\n    import urllib.request\n    import xml.etree.ElementTree as ET\n    import tarfile\n    import requests\n    import tiledb\n    import glob\n    \n    now = datetime.now()\n    ctx = tiledb.Ctx({\"vfs.s3.region\": bucket_region})\n    vfs = tiledb.VFS(ctx=ctx)\n    \n    # Initialize the file history log for the job\n    new_file_path = f\"{object_directory}/file_history_{job_id}\"\n    with vfs.open(new_file_path, 'wb') as f:\n        date_time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n        log_entry = f\"Run launched at {date_time_str}\"\n        f.write(log_entry + '\\n')  # Ensure writing bytes (use `.encode('utf-8')` if needed)\n    \n    start_time = time.time()\n\n    # Function to convert PMID to PMCID and handle ingestion\n    def convert_pmid_to_pmcid_and_ingest(pmid):\n        \"\"\"\n        Convert PMID to PMCID and attempt to download article or abstract.\n        \n        :param pmid: PubMed ID for the article.\n        \"\"\"\n        file_name = os.path.join(object_directory, f\"{pmid}_abstract.txt\")\n        url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n        params = {\n            \"db\": \"pubmed\",\n            \"id\": pmid,\n            \"retmode\": \"json\",\n            \"tool\": \"your_tool_name\",\n            \"email\": \"your_email@example.com\"\n        }\n\n        max_retries = 5\n        retry_delay = 5\n\n        for attempt in range(max_retries):\n            try:\n                response = requests.get(url, params=params)\n                if response.status_code == 200:\n                    data = response.json()\n                    pmid_str = str(pmid)\n\n                    if \"result\" in data and pmid_str in data[\"result\"]:\n                        result = data[\"result\"][pmid_str]\n                        pmcid = None\n\n                        for article_id in result.get('articleids', []):\n                            if article_id['idtype'] == 'pmc':\n                                pmcid = article_id['value']\n                                print(f\"Attempting to download the article for {pmcid}\")\n                                \n                                if download_pmc_article(pmcid):\n                                    add_to_new_file(f\"{pmcid}.pdf\")\n                                    return\n                                \n                        print(f\"No PMCID found, fetching abstract for PMID {pmid}\")\n                        if fetch_abstract(pmid):\n                            add_to_new_file(f\"{pmid}_abstract.txt\")\n                            return\n\n                    print(f\"Invalid response for PMID {pmid}.\")\n                elif response.status_code == 429:\n                    print(f\"Rate limit hit for PMID {pmid}, retrying in {retry_delay} seconds...\")\n                    time.sleep(retry_delay)\n                    retry_delay *= 2  # Exponential backoff\n\n            except requests.exceptions.ConnectionError as e:\n                print(f\"Network error on attempt {attempt + 1}: {e}\")\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n        print(f\"Failed to retrieve data for PMID {pmid} after {max_retries} attempts.\")\n\n    # Download PMCID article using the OA API\n    def download_pmc_article(pmcid):\n        \"\"\"\n        Download the article package by PMCID and upload the PDF using TileDB VFS.\n        \n        :param pmcid: PubMed Central ID.\n        \"\"\"\n        file_name = os.path.join(object_directory, f\"{pmcid}.pdf\")\n        if vfs.is_file(file_name):\n            print(f\"File {pmcid}.pdf already exists in {object_directory}\")\n            return True\n\n        url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmcid}\"\n        print(f\"Fetching data from {url}...\")\n        \n        max_retries = 5\n        retry_delay = 5\n\n        for attempt in range(max_retries):\n            try:\n                response = urllib.request.urlopen(url)\n                response_content = response.read()\n\n                if response.getcode() == 200:\n                    root = ET.fromstring(response_content)\n                    error_element = root.find('.//error')\n\n                    if error_element is not None:\n                        error_code = error_element.get('code')\n                        if error_code == 'idIsNotOpenAccess':\n                            print(f\"PMCID {pmcid} is not Open Access. We will attempt to download the abstract for {pmid}\")\n                            return False\n                    else:\n                        # Proceed to download and extract article package\n                        print(\"Attempting to download and extract the PMC article\")\n                        return download_and_extract_article(root, pmcid)\n                elif response.status_code == 429:\n                    print(f\"Rate limit hit for PMCID {pmid}, retrying in {retry_delay} seconds...\")\n                    time.sleep(retry_delay)\n                    retry_delay *= 2  # Exponential backoff\n\n            except requests.exceptions.ConnectionError as e:\n                print(f\"Network error on attempt {attempt + 1}: {e}\")\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n        print(f\"Failed to retrieve article for PMCID {pmcid} after {max_retries} attempts.\")\n        return False\n\n    def download_and_extract_article(root, pmcid):\n        \"\"\"\n        Download and extract the article tarball.\n        \n        :param root: XML root element.\n        :param pmcid: PubMed Central ID.\n        \"\"\"\n        import shutil\n        records = root.find('records')\n        if records is not None:\n            record = records.find(f'record[@id=\"{pmcid}\"]')\n            link = record.find('link[@format=\"tgz\"]')\n            if link is not None:\n                tar_url = link.get('href')\n                tar_file_name = f\"{pmcid}.tar.gz\"\n\n                print(f\"Downloading {tar_url}...\")\n                urllib.request.urlretrieve(tar_url, tar_file_name)\n\n                print(f\"Extracting {tar_file_name}...\")\n                with tarfile.open(tar_file_name, 'r:gz') as tar:\n                    tar.extractall(pmcid)\n                os.remove(tar_file_name)\n\n                os.chdir(pmcid)\n                pdf_files = glob.glob(\"**/*.pdf\", recursive=True)\n                if not pdf_files:\n                    print(f\"No PDF found in {os.getcwd()}\")\n                    os.chdir(\"..\")\n                    os.rmdir(pmcid)\n                    return False\n\n                pdf_file = pdf_files[0]\n                print(f\"Found PDF: {pdf_file}\")\n\n                with open(pdf_file, 'rb') as local_pdf_file:\n                    pdf_content = local_pdf_file.read()\n\n                with vfs.open(os.path.join(object_directory, f\"{pmcid}.pdf\"), 'wb') as vfs_pdf_file:\n                    vfs_pdf_file.write(pdf_content)\n\n                print(f\"PDF {pdf_file} successfully written to TileDB VFS.\")\n                os.chdir(\"..\")\n                shutil.rmtree(pmcid)\n                return True\n        print(f\"No records found for PMCID {pmcid}.\")\n        return False\n\n    def fetch_abstract(pmid):\n        \"\"\"\n        Fetch the abstract from PubMed by PMID.\n        \n        :param pmid: PubMed ID.\n        \"\"\"\n        url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n        params = {'db': 'pubmed', 'id': pmid, 'retmode': 'xml'}\n        \n        file_name = os.path.join(object_directory, f\"{pmid}_abstract.txt\")\n        if vfs.is_file(file_name):\n            print(f\"Abstract {pmid}_abstract.txt already exists.\")\n            return True\n\n        print(f\"Fetching abstract for PMID {pmid}...\")\n        max_retries = 5\n        retry_delay = 5\n\n        for attempt in range(max_retries):\n            try:\n                response = requests.get(url, params=params)\n                if response.status_code == 200:\n                    root = ET.fromstring(response.content)\n                    abstract_text = \"\\n\".join([abstract.text for abstract in root.findall(\".//AbstractText\")])\n\n                    if not abstract_text.strip():\n                        print(f\"No abstract available for PMID {pmid}.\")\n                        return False\n\n                    with vfs.open(file_name, 'wb') as file:\n                        file.write(abstract_text.encode('UTF-8'))\n\n                    print(f\"Abstract saved to {file_name}.\")\n                    return True\n\n                elif response.status_code == 429:\n                    print(f\"Rate limit hit for PMID {pmid}, retrying in {retry_delay} seconds...\")\n                    time.sleep(retry_delay)\n                    retry_delay *= 2\n\n            except requests.exceptions.ConnectionError as e:\n                print(f\"Network error on attempt {attempt + 1}: {e}\")\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n        print(f\"Failed to retrieve abstract for PMID {pmid} after {max_retries} attempts.\")\n        return False\n\n    def add_to_new_file(file_name):\n        \"\"\"\n        Append the name of the processed file to the file history log in S3.\n        \n        :param file_name: The file name to append to the log.\n        \"\"\"\n        print(f\"Appending {file_name} to the file history.\")\n        new_file_path = f\"{object_directory}/file_history_{job_id}\"\n        tmp_data = b\"\"  # Use bytes for consistency\n\n        try:\n            if vfs.is_file(new_file_path):\n                with vfs.open(new_file_path, 'rb') as f:\n                    tmp_data = f.read()\n\n            file_name_as_bytes = (file_name + '\\n').encode('utf-8')\n            tmp_data += file_name_as_bytes\n\n            with vfs.open(new_file_path, 'wb') as f:\n                f.write(tmp_data)\n            print(f\"Appended {file_name} to {new_file_path}\")\n        except Exception as e:\n            print(f\"Error appending to file history: {e}\")\n\n    # Process each row in the DataFrame\n    job_df.dropna(subset=['PMID Gene-disease'], inplace=True)\n    job_df.reset_index(drop=True, inplace=True)\n    total_entries = len(job_df)\n    print(f\"Total entries: {total_entries}\")\n\n    for index, row in job_df.iterrows():\n        try:\n            pmid = str(int(row[\"PMID Gene-disease\"]))\n            convert_pmid_to_pmcid_and_ingest(pmid)\n        except ValueError as e:\n            print(f\"Error processing row {index}: {e}\")\n            continue\n\n    # End timing\n    total_time = time.time() - start_time\n    total_time_minutes = total_time / 60\n    print(f\"Processed {total_entries} in {total_time:.2f} seconds or {total_time_minutes:.2f} minutes\")\n", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "c0954fae-8a82-42cb-81b0-c55a67c619a7"}, {"cell_type": "markdown", "source": "## Pipeline Factory Function\nThe `initialize_step` will build out our DAG starting with the ingestion steps. The below code: (write when it works)", "metadata": {}, "id": "95779637-e9ae-49a8-b384-08c9655feaaf"}, {"cell_type": "code", "source": "def pipeline_step(access_credentials: str, repo_url: str, out_directory: str, bucket_path: str, in_file: str = \"rag-article-list.txt\", num_jobs: int = 4, bucket_region: str = \"us-west-2\", hash_check: bool = True):\n    \"\"\"\n    Pipeline step to pull the repo, check for changes using a hash, and divide the input file into jobs for batch processing.\n    \n    :param access_credentials: Credentials to access TileDB Cloud for DAG execution.\n    :param repo_url: Git repository URL to clone or pull the latest version from.\n    :param out_directory: Output directory where the files will be stored.\n    :param bucket_path: S3 bucket path for file storage.\n    :param in_file: Input file name, defaulting to 'rag-article-list.txt'.\n    :param num_jobs: Number of jobs to divide the input file into, default is 4.\n    :param bucket_region: S3 bucket region, default is 'us-west-2'.\n    :param hash_check: If True, the function checks if the file has changed using a hash before proceeding.\n    \"\"\"\n    \n    # Step 1: Pull the latest changes from the repository\n    home_directory = os.path.expanduser(\"~\")  # Get the user's home directory\n    os.chdir(home_directory)  # Change to home directory\n    \n    repo_name = repo_url.split('/')[-1].replace('.git', '')  # Extract repo name from URL\n\n    # Check if the repo exists locally; if it does, pull the latest changes, otherwise clone it\n    if os.path.exists(repo_name):\n        os.chdir(repo_name)\n        subprocess.run([\"git\", \"pull\"])  # Pull latest changes\n    else:\n        subprocess.run([\"git\", \"clone\", repo_url])  # Clone the repo if it doesn't exist\n        os.chdir(repo_name)\n\n    # Step 2: Prepare paths and file hashes for comparison\n    full_bucket_path = f\"{bucket_path}/{out_directory}\"  # Full path to the S3 bucket\n    previous_hash_path = \"previous_hash.txt\"  # Path to the previous hash file\n    current_hash = \"\"  # Initialize current hash variable\n    current_hash_path = \"hash.txt\"  # Path to the current hash file\n\n    # Step 2: Check if the current hash file exists\n    if os.path.exists(current_hash_path):\n        # If the hash file exists, read the current hash value\n        with open(current_hash_path, 'r') as f:\n            current_hash = f.read().strip()\n            print(f\"Current hash: {current_hash}\")\n    else:\n        # If no current hash file exists, create one by hashing the input file\n        print(f\"{current_hash_path} doesn't exist. Let's create it.\")\n        if os.path.exists(in_file):\n            current_hash = hash_file(in_file)  # Compute the hash of the input file\n            with open(current_hash_path, 'w') as fh:\n                fh.write(current_hash)  # Write the current hash to the hash file\n        else:\n            print(f\"{in_file} does NOT exist. Please submit a valid file and rerun this function.\")\n            return  # Exit the function if the input file does not exist\n\n    # Step 3: Check for the previous hash and compare it to the current hash\n    if os.path.exists(previous_hash_path):\n        with open(previous_hash_path, 'r') as f:\n            previous_hash = f.read().strip()  # Read the previous hash\n            if current_hash == previous_hash:\n                print(\"File did not change.\")  # If the hash matches, the file hasn't changed\n                if hash_check:\n                    exit(1)  # Stop if hash_check is True and file hasn't changed\n                else:\n                    print(\"Continuing with processing despite no changes.\")\n            else:\n                # If the hash differs, update the previous hash and continue processing\n                print(\"Input file has changed, proceeding with processing.\")\n                with open(previous_hash_path, 'w') as fh:\n                    fh.write(current_hash)  # Write the new hash to the previous hash file\n    else:\n        # If no previous hash exists, create one and proceed\n        print(\"No previous hash found. Creating one now.\")\n        with open(previous_hash_path, 'w') as fh:\n            fh.write(current_hash)\n\n    # Step 4: Commit the updated hash and file to the Git repository\n    add_and_commit_files(\"updating the hash and file to keep repo up to date\")  # Commit changes to the repo\n\n    # Step 5: Load the DataFrame from the input file\n    df = pd.read_csv(in_file, sep='\\t', engine='python')  # Load input file into a DataFrame\n    print(\"Available columns in the DataFrame:\")\n    print(df.columns)  # Print the available columns in the DataFrame for debugging\n\n    # Step 6: Divide the DataFrame into jobs based on the number of user-defined jobs\n    total_rows = len(df)  # Get the total number of rows in the DataFrame\n    print(f\"Total rows in the DataFrame: {total_rows}\")\n\n    # Ensure that num_jobs does not exceed the number of rows\n    if num_jobs > total_rows:\n        num_jobs = total_rows  # If there are more jobs than rows, reduce num_jobs to match the rows\n        print(f\"Number of jobs reduced to {num_jobs} to match total rows.\")\n\n    # Calculate job size: distribute rows as evenly as possible\n    job_size = math.ceil(total_rows / num_jobs)\n\n    # Split the DataFrame into chunks (jobs) of approximately equal size\n    jobs = [df.iloc[i:i + job_size] for i in range(0, total_rows, job_size)]\n\n    print(f\"Divided into {len(jobs)} jobs\")  # Print the number of jobs created\n\n    # Step 7: Set up TileDB Cloud DAG for batch processing\n    dag = tiledb.cloud.dag.DAG(name=\"document_batch\", mode=tiledb.cloud.dag.Mode.BATCH)  # Initialize a DAG\n    consolidate_node = dag.submit(consolidate_chunks, num_jobs, bucket_region, full_bucket_path, access_credentials_name=access_credentials)\n    delete_unwanted_node = dag.submit(delete_unwanted_files, bucket_region, full_bucket_path, access_credentials_name=access_credentials)\n    delete_unwanted_node.depends_on(consolidate_node)  # Set dependency so delete_unwanted_node waits for consolidate_node\n\n    # Step 8: Submit each job for ingestion\n    i = 0  # Initialize a counter for job IDs\n    for job_df in jobs:\n        if job_df.empty:\n            print(\"Skipping empty job.\")  # Skip empty jobs\n            continue\n        # Estimate CPU and memory resources for each job\n        cpu, mem = estimate_resources(job_df)\n        print(f\"Processing {len(job_df)} rows, estimated CPU: {cpu}, estimated memory: {mem:.2f} GB.\")\n\n        # Submit each job to the TileDB Cloud DAG for ingestion\n        ingest_node = dag.submit(\n            pmid_ingestion,\n            job_df,\n            full_bucket_path,\n            bucket_region,\n            i,\n            access_credentials_name=access_credentials,\n            resources={\"cpu\": f\"{cpu}\", \"memory\": f\"{mem}Gi\"}  # Set resources for the job\n        )\n        consolidate_node.depends_on(ingest_node)  # Make consolidate_node dependent on this ingestion job\n        i += 1  # Increment the job ID\n\n    delete_unwanted_node.depends_on(consolidate_node)  # Ensure delete_unwanted_node runs after consolidation\n    dag.compute()  # Compute the DAG\n    dag.wait()  # Wait for all DAG tasks to complete\n    print(f\"Finished processing all jobs, consolidating history, and cleaning up undesired S3 files at {full_bucket_path}\")\n", "metadata": {"trusted": true}, "execution_count": 3, "outputs": [], "id": "82981466-efde-43e1-829e-5eb689428bc4"}, {"cell_type": "code", "source": "#def initialize_step(access_credentials: str,repo_url: str, out_directory: str, bucket_path: str, in_file: str = \"rag-article-list.txt\", num_jobs: int = 4, bucket_region: str = \"us-west-2\", hash_check: bool = True)\n\npipeline_step(repo_url=\"https://github.com/TileDB-Inc/pmc-llm.git\", out_directory=\"pmc/rag/ingestion\", in_file=\"rag-article-list.txt\", access_credentials=\"chase-bucket-access\", bucket_path=\"s3://chase-cloud\",hash_check=False,num_jobs=40)", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "b8018d9d-e74a-41d8-a50b-f8504df14f67"}, {"cell_type": "markdown", "source": "# TileDB Ingestion Pipeline", "metadata": {}, "id": "dbecc262-8b80-48ac-86df-c698cb769764"}, {"cell_type": "code", "source": "# Optional: Download the consolidated file from S3 and write it to your local Git repository\n# Set up TileDB context with S3 credentials (replace with your actual credentials)\nctx = tiledb.Ctx({\n    \"vfs.s3.region\": \"us-west-2\",  # S3 region\n    \"vfs.s3.aws_access_key_id\": \"\",  # AWS Access Key (add your key here)\n    \"vfs.s3.aws_secret_access_key\": \"\"  # AWS Secret Access Key (add your key here)\n})\n\n# S3 path to the consolidated file\nconsolidated_file_path = \"s3://chase-cloud/pmc/rag/ingestion/consolidated_file_history\"\n\n# Initialize TileDB VFS to interact with S3\nvfs = tiledb.VFS(ctx=ctx)\n\n# Step 1: Read the contents of the consolidated file from S3\nfile_contents = \"\"\nwith vfs.open(consolidated_file_path, 'rb') as f:  # Open the file in binary read mode\n    file_contents = f.read()  # Read the entire file contents\n\n# Step 2: Write the contents to the local file system in the current directory\n# You may want to navigate to your Git repository directory before writing\n# Uncomment and modify the following line if needed:\n# os.chdir(\"pmc-llm\")  # Change to your Git repository directory\n\n# Create the local path for the consolidated file\npath = os.path.join(os.getcwd(), \"consolidated_file_history\")  # Store in the current working directory\n\n# Write the contents from S3 to the local file in binary write mode\nwith open(path, 'wb') as f:\n    f.write(file_contents)  # Write the downloaded file contents to the local file\n\n# Step 3: Assuming `add_and_commit_files` is a valid function, commit the changes to Git\n# This function should add the file, commit it with a message, and push the changes\nadd_and_commit_files(\"updating the consolidated file history for tracking purposes\")\n", "metadata": {"trusted": true}, "execution_count": 20, "outputs": [{"name": "stdout", "text": "Working direcotry is /home/jovyan/pmc-llm\n[main ba25068] updating the hash and file to keep repo up to date\n 1 file changed, 1668 insertions(+)\n create mode 100644 consolidated_file_history\n", "output_type": "stream"}, {"name": "stderr", "text": "To https://github.com/TileDB-Inc/pmc-llm.git\n   679f72f..ba25068  main -> main\n", "output_type": "stream"}], "id": "d6525fcd-dd25-4414-8f7c-5e50e8464bc9"}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "77a728de-7c4c-4361-bb2e-631127af2b43"}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "5867ac86-8454-4f87-ac69-983a4a72a993"}, {"cell_type": "markdown", "source": "", "metadata": {}, "id": "9389f1b6-ed06-49d6-8f77-850d055673b7"}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "ae8530d0-7c9b-4cbb-a3f5-7c0ae6cfa0ae"}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "371c155b-890e-4664-beb2-28a6044d5c1a"}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "execution_count": null, "outputs": [], "id": "fc637f5e-43d8-411d-ba4e-419266717aa4"}]}